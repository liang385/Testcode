完全分布式的搭建:
1:将分布式的虚拟机进行克隆（3台）
2：修改hosts文件   vim /etc/hosts  和hostname分别改为对应的主名

196.168.126.122   hadoop00
196.168.126.123   hadoop01
196.168.126.123   hadoop02

3:修改Hadoop核心配置文件（需要先进行集群规划）
cd /hadoop/etc/hadoop


3.1: vim core-site.xml
<configuration>
    <!-- NameNode的地址 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop00:8020</value>
    </property>
    <!-- 数据存放目录 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/app/hadoop/data</value>
    </property>
    <!-- 配置HDFS网页登录使用的静态用户为root -->
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>root</value>
    </property>
</configuration>

3.2 vim hdfs-site.xml
<configuration>
    <!-- NameNode web端访问地址 -->
    <property>
        <name>dfs.namenode.http-address</name>
        <value>hadoop00:9870</value>
    </property>
    <!-- SecondaryNameNode web端访问地址 -->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoop01:9868</value>
    </property>
</configuration>

3.3 vim yarn-site.xml
<configuration>
    <!-- 指定MR执行shuffle -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <!-- 指定ResourceManager的地址-->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop01</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>

3.4 vim mapred-site.xml
<configuration>
    <!-- 指定MapReduce程序运行在Yarn上 -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>

3.5 vim workers(hadoop3版本)
hadoop00
hadoop01
hadoop02

4:实现3台虚拟机的互相ssh免密登录
ssh-keygen -t rsa
[root@hadoop00 ~]$ ssh-copy-id hadoop00
[root@hadoop00 ~]$ ssh-copy-id hadoop01
[root@hadoop00 ~]$ ssh-copy-id hadoop02

5:将Hadoop分发给hadoop01,hadoop02
rsync -av /opt/app/hadoop root@hadoop01:/opt/app/

6:格式化Namedone
hdfs namenode -format

7:启动集群在Namedone上
start-dfs.sh

8：游览器访问hdfs
192.168.126.122:9870